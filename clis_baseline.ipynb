{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clis_baseline.ipynb",
      "provenance": [],
      "mount_file_id": "1oGmIR8_G7AUm7iDhP3AzVcW_0k6PdCT5",
      "authorship_tag": "ABX9TyMMeXu3U4NxJI1Nkm1vkfl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natureLanguageQing/BERT-BiLSTM-CRF-NER/blob/master/clis_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-L-HrUdt_GQ",
        "colab_type": "code",
        "outputId": "9d32ae44-b839-4db7-a03f-d75ff190d0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!git clone https://github.com/bojone/lic2020_baselines.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lic2020_baselines'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 39 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdgT91yRuNC9",
        "colab_type": "code",
        "outputId": "2a4b2f0e-70e0-4ed8-8f5a-c5e1ce693f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXWHjnHuWFh",
        "colab_type": "code",
        "outputId": "8023f3e5-6118-406c-fe8b-7d3cecfc64b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!wget http://bj.bcebos.com/v1/ai-studio-online/e521ab850c7046669d576e2c0c2794666c47c622db1d4484bbbe3e7bed5da8ae?responseContentDisposition=attachment%3B%20filename%3Dsample5.json.zip&authorization=bce-auth-v1%2F0ef6765c1e494918bc0d4c3ca3e5c6d1%2F2020-03-09T10%3A33%3A23Z%2F-1%2F%2F197fcd92c227979f99526c0027081fe27d17d6c0fad3577afdaac63ba7960a4d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-31 13:13:31--  http://bj.bcebos.com/v1/ai-studio-online/e521ab850c7046669d576e2c0c2794666c47c622db1d4484bbbe3e7bed5da8ae?responseContentDisposition=attachment%3B%20filename%3Dsample5.json.zip\n",
            "Resolving bj.bcebos.com (bj.bcebos.com)... 103.235.46.61\n",
            "Connecting to bj.bcebos.com (bj.bcebos.com)|103.235.46.61|:80... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2020-03-31 13:13:33 ERROR 403: Forbidden.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2c2jjbnunyS",
        "colab_type": "code",
        "outputId": "fba4da05-4a5d-4aff-c30b-3290a93cc9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  1 12:14:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDTtotwlunsl",
        "colab_type": "code",
        "outputId": "5d711761-6f8d-42f9-bf73-6805f871ea9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!unzip train_data.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train_data.json.zip\n",
            "   creating: train_data/\n",
            "  inflating: train_data/train.json   \n",
            "  inflating: train_data/License.docx  \n",
            "   creating: __MACOSX/train_data/\n",
            "  inflating: __MACOSX/train_data/._License.docx  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t71XSGL1s8XX",
        "colab_type": "code",
        "outputId": "dc6bceea-c409-4b4e-d02f-e40a12068f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!unzip dev_data.json.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  dev_data.json.zip\n",
            "   creating: dev_data/\n",
            "  inflating: dev_data/dev.json       \n",
            "  inflating: dev_data/License.docx   \n",
            "   creating: __MACOSX/dev_data/\n",
            "  inflating: __MACOSX/dev_data/._License.docx  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYex8L53uwpt",
        "colab_type": "code",
        "outputId": "175a1231-a8c2-4725-f727-670cdbd57ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!unzip /content/event_schema.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/event_schema.zip\n",
            "   creating: event_schema/\n",
            "  inflating: event_schema/.DS_Store  \n",
            "   creating: __MACOSX/event_schema/\n",
            "  inflating: __MACOSX/event_schema/._.DS_Store  \n",
            "  inflating: event_schema/License.docx  \n",
            "  inflating: __MACOSX/event_schema/._License.docx  \n",
            "  inflating: event_schema/event_schema.pdf  \n",
            "  inflating: __MACOSX/event_schema/._event_schema.pdf  \n",
            "  inflating: event_schema/event_schema.json  \n",
            "  inflating: __MACOSX/event_schema/._event_schema.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gj_z3ylsT1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -i /content/drive/My\\ Drive/electra_tiny.zip（副本） file2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIqJszVtHT0",
        "colab_type": "code",
        "outputId": "696432a4-b466-4e8c-f3fa-d0a8a88bf0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert4keras\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/11/0beefa17c2afb2962fda5021824b6f8dd0e0bb20fc4c7d71b0e1e40106c9/bert4keras-0.6.9.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.18.2)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.6.9-cp36-none-any.whl size=33997 sha256=abf5785ca20bfd1a5edf6c7b53ad1505e82748834afe9dc6ad8b50212afcd5eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/8b/04/0d1b06be2562f3e80a83cc6fd86257ed45e568a90293a9ab20\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH0fCst8shxE",
        "colab_type": "code",
        "outputId": "26abfd02-3160-470f-d66f-e8021da0a774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!unzip file2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  file2\n",
            "  inflating: bert_config_tiny.json   \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._bert_config_tiny.json  \n",
            "  inflating: model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: model.ckpt-1000000.index  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.index  \n",
            "  inflating: model.ckpt-1000000.meta  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.meta  \n",
            "  inflating: vocab.txt               \n",
            "  inflating: __MACOSX/._vocab.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWrU9rxWu8EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "# 百度LIC2020的事件抽取赛道，非官方baseline\n",
        "# 直接用BERT+CRF\n",
        "# 在第一期测试集上能达到0.73的F1，略优于官方baseline\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from bert4keras.layers import ConditionalRandomField\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from tqdm import tqdm\n",
        "import pylcs\n",
        "\n",
        "# 基本信息\n",
        "maxlen = 128\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5\n",
        "crf_lr_multiplier = 1000  # 必要时扩大CRF层的学习率\n",
        "\n",
        "# bert配置\n",
        "config_path = 'bert_config_tiny.json'\n",
        "checkpoint_path = 'model.ckpt-1000000'\n",
        "dict_path = 'vocab.txt'\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    D = []\n",
        "    with open(filename) as f:\n",
        "        for l in f:\n",
        "            l = json.loads(l)\n",
        "            arguments = {}\n",
        "            for event in l['event_list']:\n",
        "                for argument in event['arguments']:\n",
        "                    key = argument['argument']\n",
        "                    value = (event['event_type'], argument['role'])\n",
        "                    arguments[key] = value\n",
        "            D.append((l['text'], arguments))\n",
        "    return D\n",
        "\n",
        "\n",
        "# 读取数据\n",
        "train_data = load_data('train_data/train.json')\n",
        "valid_data = load_data('dev_data/dev.json')\n",
        "\n",
        "# 读取schema\n",
        "with open('event_schema/event_schema.json') as f:\n",
        "    id2label, label2id, n = {}, {}, 0\n",
        "    for l in f:\n",
        "        l = json.loads(l)\n",
        "        for role in l['role_list']:\n",
        "            key = (l['event_type'], role['role'])\n",
        "            id2label[n] = key\n",
        "            label2id[key] = n\n",
        "            n += 1\n",
        "    num_labels = len(id2label) * 2 + 1\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
        "\n",
        "\n",
        "def search(pattern, sequence):\n",
        "    \"\"\"从sequence中寻找子串pattern\n",
        "    如果找到，返回第一个下标；否则返回-1。\n",
        "    \"\"\"\n",
        "    n = len(pattern)\n",
        "    for i in range(len(sequence)):\n",
        "        if sequence[i:i + n] == pattern:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, (text, arguments) in self.sample(random):\n",
        "            token_ids, segment_ids = tokenizer.encode(text, max_length=maxlen)\n",
        "            labels = [0] * len(token_ids)\n",
        "            for argument in arguments.items():\n",
        "                a_token_ids = tokenizer.encode(argument[0])[0][1:-1]\n",
        "                start_index = search(a_token_ids, token_ids)\n",
        "                if start_index != -1:\n",
        "                    labels[start_index] = label2id[argument[1]] * 2 + 1\n",
        "                    for i in range(1, len(a_token_ids)):\n",
        "                        labels[start_index + i] = label2id[argument[1]] * 2 + 2\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append(labels)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = sequence_padding(batch_labels)\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "\n",
        "\n",
        "model = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    model = 'electra'\n",
        ")\n",
        "\n",
        "output = Dense(num_labels)(model.output)\n",
        "CRF = ConditionalRandomField(lr_multiplier=crf_lr_multiplier)\n",
        "output = CRF(output)\n",
        "\n",
        "model = Model(model.input, output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=CRF.sparse_loss,\n",
        "    optimizer=Adam(learning_rate),\n",
        "    metrics=[CRF.sparse_accuracy]\n",
        ")\n",
        "\n",
        "\n",
        "def viterbi_decode(nodes, trans):\n",
        "    \"\"\"Viterbi算法求最优路径\n",
        "    其中nodes.shape=[seq_len, num_labels],\n",
        "        trans.shape=[num_labels, num_labels].\n",
        "    \"\"\"\n",
        "    labels = np.arange(num_labels).reshape((1, -1))\n",
        "    scores = nodes[0].reshape((-1, 1))\n",
        "    scores[1:] -= np.inf  # 第一个标签必然是0\n",
        "    paths = labels\n",
        "    for l in range(1, len(nodes)):\n",
        "        M = scores + trans + nodes[l].reshape((1, -1))\n",
        "        idxs = M.argmax(0)\n",
        "        scores = M.max(0).reshape((-1, 1))\n",
        "        paths = np.concatenate([paths[:, idxs], labels], 0)\n",
        "    return paths[:, scores[0].argmax()]\n",
        "\n",
        "\n",
        "def extract_arguments(text):\n",
        "    \"\"\"命名实体识别函数\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    while len(tokens) > 512:\n",
        "        tokens.pop(-2)\n",
        "    token_ids = tokenizer.tokens_to_ids(tokens)\n",
        "    segment_ids = [0] * len(token_ids)\n",
        "    nodes = model.predict([[token_ids], [segment_ids]])[0]\n",
        "    trans = K.eval(CRF.trans)\n",
        "    labels = viterbi_decode(nodes, trans)[1:-1]\n",
        "    arguments, starting = [], False\n",
        "    for token, label in zip(tokens[1:-1], labels):\n",
        "        if label > 0:\n",
        "            if label % 2 == 1:\n",
        "                starting = True\n",
        "                arguments.append([[token], id2label[(label - 1) // 2]])\n",
        "            elif starting:\n",
        "                arguments[-1][0].append(token)\n",
        "            else:\n",
        "                starting = False\n",
        "        else:\n",
        "            starting = False\n",
        "\n",
        "    return {tokenizer.decode(w, w): l for w, l in arguments}\n",
        "\n",
        "\n",
        "def evaluate(data):\n",
        "    \"\"\"评测函数（跟官方评测结果不一定相同，但很接近）\n",
        "    \"\"\"\n",
        "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
        "    for text, arguments in tqdm(data):\n",
        "        inv_arguments = {v: k for k, v in arguments.items()}\n",
        "        pred_arguments = extract_arguments(text)\n",
        "        pred_inv_arguments = {v: k for k, v in pred_arguments.items()}\n",
        "        Y += len(pred_inv_arguments)\n",
        "        Z += len(inv_arguments)\n",
        "        for k, v in pred_inv_arguments.items():\n",
        "            if k in inv_arguments:\n",
        "                # 用最长公共子串作为匹配程度度量\n",
        "                l = pylcs.lcs(v, inv_arguments[k])\n",
        "                X += 2. * l / (len(v) + len(inv_arguments[k]))\n",
        "    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
        "    return f1, precision, recall\n",
        "\n",
        "\n",
        "def predict_to_file(in_file, out_file):\n",
        "    \"\"\"预测结果到文件，方便提交\n",
        "    \"\"\"\n",
        "    fw = open(out_file, 'w', encoding='utf-8')\n",
        "    with open(in_file) as fr:\n",
        "        for l in tqdm(fr):\n",
        "            l = json.loads(l)\n",
        "            arguments = extract_arguments(l['text'])\n",
        "            event_list = []\n",
        "            for k, v in arguments.items():\n",
        "                event_list.append({\n",
        "                    'event_type': v[0],\n",
        "                    'arguments': [{\n",
        "                        'role': v[1],\n",
        "                        'argument': k\n",
        "                    }]\n",
        "                })\n",
        "            l['event_list'] = event_list\n",
        "            l = json.dumps(l, ensure_ascii=False)\n",
        "            fw.write(l + '\\n')\n",
        "    fw.close()\n",
        "\n",
        "\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    \"\"\"评估和保存模型\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.best_val_f1 = 0.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        f1, precision, recall = evaluate(valid_data)\n",
        "        if f1 >= self.best_val_f1:\n",
        "            self.best_val_f1 = f1\n",
        "            model.save_weights('best_model.weights')\n",
        "        print(\n",
        "            'f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
        "            (f1, precision, recall, self.best_val_f1)\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_generator = data_generator(train_data, batch_size)\n",
        "    evaluator = Evaluator()\n",
        "\n",
        "    model.fit_generator(\n",
        "        train_generator.forfit(),\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=epochs,\n",
        "        callbacks=[evaluator]\n",
        "    )\n",
        "\n",
        "else:\n",
        "\n",
        "    model.load_weights('best_model.weights')\n",
        "    # predict_to_file('/root/baidu/datasets/ee/test1_data/test1.json', 'ee_pred.json')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84rCsCXgtii1",
        "colab_type": "code",
        "outputId": "b1f4c06a-8404-4d54-b059-a9116b1e5c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "!pip install pylcs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pylcs\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/3d/1bcd4daca7fb70311aa46507b74bca8f7ec414ee2d0f8a4d5e0ea2084163/pylcs-0.0.6.tar.gz\n",
            "Collecting pybind11>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl (296kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 3.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pylcs\n",
            "  Building wheel for pylcs (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pylcs\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pylcs\n",
            "Failed to build pylcs\n",
            "Installing collected packages: pybind11, pylcs\n",
            "    Running setup.py install for pylcs ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed pybind11-2.5.0 pylcs-0.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9xzBMQMwRAz",
        "colab_type": "code",
        "outputId": "fa88e61e-a5cb-423f-9d3f-4dd963fed5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -y tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25AH-oGAwxuy",
        "colab_type": "code",
        "outputId": "c27afdc1-b2b0-4e46-e08e-65d1ff9c4012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 44kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1BVDSDwJwj",
        "colab_type": "code",
        "outputId": "3960d21c-659a-4790-86ff-3befad2a9b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!unzip /content/event_schema.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/event_schema.zip, /content/event_schema.zip.zip or /content/event_schema.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHcY53aywAIs",
        "colab_type": "code",
        "outputId": "35a56ce1-edba-47d5-a19b-2048c806ecc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "!pip install pylcs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pylcs\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/3d/1bcd4daca7fb70311aa46507b74bca8f7ec414ee2d0f8a4d5e0ea2084163/pylcs-0.0.6.tar.gz\n",
            "Collecting pybind11>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl (296kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pylcs\n",
            "  Building wheel for pylcs (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pylcs\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pylcs\n",
            "Failed to build pylcs\n",
            "Installing collected packages: pybind11, pylcs\n",
            "    Running setup.py install for pylcs ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed pybind11-2.5.0 pylcs-0.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDzvq61FgjtG",
        "colab_type": "code",
        "outputId": "dbe783ac-6563-4e8b-ada8-71a344ad96fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import os\n",
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbeddingV2\n",
        "from kashgari.tokenizer import BertTokenizer\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "kashgari.config.use_cudnn_cell = True\n",
        "model_folder = 'electra_tiny'\n",
        "\n",
        "checkpoint_path = os.path.join('model.ckpt-1000000')\n",
        "config_path = os.path.join('bert_config_tiny.json')\n",
        "vocab_path = os.path.join('vocab.txt')\n",
        "\n",
        "def data_doubles(train_data):\n",
        "    train_features = [list(i[0]) for i in train_data]\n",
        "    train_labels = [i[1] for i in train_data]\n",
        "    return train_features, train_labels\n",
        "\n",
        "\n",
        "\n",
        "def read_message(path):\n",
        "    data = pd.read_csv(path).values.tolist()\n",
        "    random_order = list(range(len(data)))\n",
        "    np.random.shuffle(random_order)\n",
        "    train_data, valid_data, test_data = [], [], []\n",
        "    for i, j in enumerate(random_order):\n",
        "        if i % 10 != 0 and i % 10 != 1:\n",
        "            train_data.append(data[i])\n",
        "        if i % 10 == 0:\n",
        "            valid_data.append(data[i])\n",
        "        if i % 10 == 1:\n",
        "            test_data.append(data[i])\n",
        "    train_features, train_labels = data_doubles(train_data)\n",
        "    valid_features, valid_labels = data_doubles(valid_data)\n",
        "    test_features, test_labels = data_doubles(test_data)\n",
        "    return train_features, train_labels, valid_features, valid_labels, test_features, test_labels\n",
        "train_features, train_labels, valid_features, valid_labels, test_features, test_labels = read_message(\"five_CLS_data.csv\")\n",
        "\n",
        "feature_list = []\n",
        "label_list = []\n",
        "max_len = 0\n",
        "# 根据数据集中的最大信息长度判断当前任务的max-len\n",
        "for i in five_CLS_data:\n",
        "    feature_list.append(i[0])\n",
        "    if len(i[0]) > max_len:\n",
        "        max_len = len(i[0])\n",
        "    label_list.append(i[1])\n",
        "tokenizer = BertTokenizer.load_from_vacab_file(vocab_path)\n",
        "embed = BERTEmbeddingV2(vocab_path,\n",
        "                        config_path,\n",
        "                        checkpoint_path,\n",
        "                        bert_type='electra',\n",
        "                        task=kashgari.CLASSIFICATION,\n",
        "                        sequence_length=max_len)\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='tf_dir', update_freq=1000)\n",
        "\n",
        "\n",
        "\n",
        "from kashgari.tasks.classification import CNNLSTMModel\n",
        "\n",
        "model = CNNLSTMModel(embed)\n",
        "\n",
        "# ------------ build model ------------\n",
        "model.fit(\n",
        "    train_features, train_labels,\n",
        "    valid_features, valid_labels,\n",
        "    epochs=30,\n",
        "    batch_size=256,\n",
        "    callbacks =[tf_board_callback]\n",
        ")\n",
        "model.evaluate(test_features, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: The 'load_from_vacab_file' function is deprecated, use 'load_from_vocab_file' instead\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:22: UserWarning: bert4keras.bert has been renamed as bert4keras.models.\n",
            "  warnings.warn('bert4keras.bert has been renamed as bert4keras.models.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:23: UserWarning: please use bert4keras.models.\n",
            "  warnings.warn('please use bert4keras.models.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:44: UserWarning: bert4keras.tokenizer has been renamed as bert4keras.tokenizers.\n",
            "  warnings.warn('bert4keras.tokenizer has been renamed as bert4keras.tokenizers.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:45: UserWarning: please use bert4keras.tokenizers.\n",
            "  warnings.warn('please use bert4keras.tokenizers.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7f94a14feb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7f94a14feb00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     (None, None, 312)    6591936     Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 312)    624         Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 312)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 312)    159744      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 312)    624         Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 312)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 312)    390624      Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 312)    624         Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 312)    750312      Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 312)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 312)    624         Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 312)    624         Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 312)    750312      Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 312)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 312)    624         Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 312)    624         Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 312)    750312      Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 312)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 312)    624         Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 312)    624         Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 312)    750312      Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 312)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 312)    624         Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "non_masking_layer_7 (NonMasking (None, None, 312)    0           Transformer-3-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 32)     29984       non_masking_layer_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, None, 32)     0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_5 (CuDNNLSTM)        (None, 100)          53600       max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_198 (Dense)               (None, 5)            505         cu_dnnlstm_5[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 11,405,753\n",
            "Trainable params: 84,089\n",
            "Non-trainable params: 11,321,664\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "38/38 [==============================] - 9s 232ms/step - loss: 1.5910 - acc: 0.2557 - val_loss: 1.5859 - val_acc: 0.2604\n",
            "Epoch 2/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 1.5847 - acc: 0.2648 - val_loss: 1.5797 - val_acc: 0.2922\n",
            "Epoch 3/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 1.4498 - acc: 0.3626 - val_loss: 1.2997 - val_acc: 0.4816\n",
            "Epoch 4/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 1.1977 - acc: 0.5152 - val_loss: 1.0906 - val_acc: 0.5751\n",
            "Epoch 5/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 1.0114 - acc: 0.6025 - val_loss: 0.9053 - val_acc: 0.6636\n",
            "Epoch 6/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.8908 - acc: 0.6592 - val_loss: 0.8241 - val_acc: 0.7062\n",
            "Epoch 7/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.8083 - acc: 0.6952 - val_loss: 0.7319 - val_acc: 0.7404\n",
            "Epoch 8/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.7138 - acc: 0.7393 - val_loss: 0.7140 - val_acc: 0.7446\n",
            "Epoch 9/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.6555 - acc: 0.7619 - val_loss: 0.6278 - val_acc: 0.7830\n",
            "Epoch 10/30\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.6117 - acc: 0.7815 - val_loss: 0.6187 - val_acc: 0.7696\n",
            "Epoch 11/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.5666 - acc: 0.7949 - val_loss: 0.5734 - val_acc: 0.8047\n",
            "Epoch 12/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.4903 - acc: 0.8264 - val_loss: 0.5264 - val_acc: 0.8247\n",
            "Epoch 13/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.4612 - acc: 0.8353 - val_loss: 0.5171 - val_acc: 0.8114\n",
            "Epoch 14/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.4613 - acc: 0.8383 - val_loss: 0.5227 - val_acc: 0.8230\n",
            "Epoch 15/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.4633 - acc: 0.8342 - val_loss: 0.4829 - val_acc: 0.8289\n",
            "Epoch 16/30\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.4042 - acc: 0.8560 - val_loss: 0.4610 - val_acc: 0.8481\n",
            "Epoch 17/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3720 - acc: 0.8720 - val_loss: 0.4740 - val_acc: 0.8364\n",
            "Epoch 18/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3680 - acc: 0.8707 - val_loss: 0.4156 - val_acc: 0.8614\n",
            "Epoch 19/30\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3242 - acc: 0.8854 - val_loss: 0.4214 - val_acc: 0.8506\n",
            "Epoch 20/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3190 - acc: 0.8881 - val_loss: 0.4350 - val_acc: 0.8614\n",
            "Epoch 21/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3363 - acc: 0.8816 - val_loss: 0.3708 - val_acc: 0.8740\n",
            "Epoch 22/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3143 - acc: 0.8919 - val_loss: 0.3761 - val_acc: 0.8773\n",
            "Epoch 23/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2882 - acc: 0.8976 - val_loss: 0.3250 - val_acc: 0.8982\n",
            "Epoch 24/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2670 - acc: 0.9048 - val_loss: 0.3391 - val_acc: 0.8856\n",
            "Epoch 25/30\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2527 - acc: 0.9124 - val_loss: 0.3340 - val_acc: 0.8881\n",
            "Epoch 26/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2630 - acc: 0.9071 - val_loss: 0.4158 - val_acc: 0.8648\n",
            "Epoch 27/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2706 - acc: 0.9061 - val_loss: 0.2883 - val_acc: 0.9082\n",
            "Epoch 28/30\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2251 - acc: 0.9241 - val_loss: 0.3572 - val_acc: 0.8923\n",
            "Epoch 29/30\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2545 - acc: 0.9146 - val_loss: 0.2856 - val_acc: 0.9149\n",
            "Epoch 30/30\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.2178 - acc: 0.9225 - val_loss: 0.2899 - val_acc: 0.9048\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          健康     0.9918    0.8736    0.9290       277\n",
            "          国际     0.9665    0.9814    0.9739       323\n",
            "          时政     0.8172    0.8941    0.8539       170\n",
            "          社会     0.8403    0.9132    0.8752       242\n",
            "          财经     0.9322    0.8871    0.9091       186\n",
            "\n",
            "    accuracy                         0.9157      1198\n",
            "   macro avg     0.9096    0.9099    0.9082      1198\n",
            "weighted avg     0.9203    0.9157    0.9165      1198\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B959k_FAlVpz",
        "colab_type": "code",
        "outputId": "76acbc2b-24ff-4fa2-d970-15ee182700fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!wget https://drive.google.com/uc?export=download&confirm=t4ju&id=1UP4byt4-kgenwST0KvyMYNbln6FfaSLp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-01 02:33:15--  https://drive.google.com/uc?export=download\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.204.100, 172.217.204.139, 172.217.204.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.204.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 400 Bad Request\n",
            "2020-04-01 02:33:15 ERROR 400: Bad Request.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-msGoZmIuL",
        "colab_type": "code",
        "outputId": "e17cb54a-2baf-437c-a396-5ea243a9c89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!wget https://drive.google.com/uc?export=download&confirm=t4ju&id=1UP4byt4-kgenwST0KvyMYNbln6FfaSLp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n",
            "--2020-04-01 02:36:32--  https://drive.google.com/uc?export=download\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.125.139, 108.177.125.101, 108.177.125.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 400 Bad Request\n",
            "2020-04-01 02:36:32 ERROR 400: Bad Request.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jX699PgjcFI",
        "colab_type": "code",
        "outputId": "c83151d7-dd7c-40a6-b9ef-39dc3c896a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZGdPZAumXe4",
        "colab_type": "code",
        "outputId": "512ffdb3-7560-4fd8-9d7d-1674db377898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/electra_tiny.zip（副本）"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/electra_tiny.zip（副本）\n",
            "  inflating: bert_config_tiny.json   \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._bert_config_tiny.json  \n",
            "  inflating: model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: model.ckpt-1000000.index  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.index  \n",
            "  inflating: model.ckpt-1000000.meta  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.meta  \n",
            "  inflating: vocab.txt               \n",
            "  inflating: __MACOSX/._vocab.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txbBGGb7il-a",
        "colab_type": "code",
        "outputId": "4d3886b7-39e8-4c81-cc39-ec0976c27e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip uninstall -y tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0rc2:\n",
            "  Successfully uninstalled tensorflow-2.2.0rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06x7gWb8ilz9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrmKc_xEv7oB",
        "colab_type": "code",
        "outputId": "b8b435ce-ceb4-47d3-c6f9-190fe0c73aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert4keras\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/11/0beefa17c2afb2962fda5021824b6f8dd0e0bb20fc4c7d71b0e1e40106c9/bert4keras-0.6.9.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.18.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (2.10.0)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.6.9-cp36-none-any.whl size=33997 sha256=32feb00f7a1e191ffd5d9a42b87fb6614aab305a73bc5740f8b6a16036ccfc09\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/8b/04/0d1b06be2562f3e80a83cc6fd86257ed45e568a90293a9ab20\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc2ns758ix6T",
        "colab_type": "code",
        "outputId": "c7a6b015-a61a-453e-f427-b08d130120e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 23.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSGPl6CUgdg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T82EEQCUhVx5",
        "colab_type": "code",
        "outputId": "93c8f427-9a54-46ed-bcac-4a8e9a188e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall kashgari && pip install git+https://github.com/BrikerMan/Kashgari\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping kashgari as it is not installed.\u001b[0m\n",
            "Collecting git+https://github.com/BrikerMan/Kashgari\n",
            "  Cloning https://github.com/BrikerMan/Kashgari to /tmp/pip-req-build-o7yxmpew\n",
            "  Running command git clone -q https://github.com/BrikerMan/Kashgari /tmp/pip-req-build-o7yxmpew\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (2.10.0)\n",
            "Collecting keras-bert>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n",
            "Collecting keras-gpt-2>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/df/19/d11eac066ffcb61ec9edd23c02e4651eaa31f1f67c167a636dd90b6142a4/keras-gpt-2-0.14.0.tar.gz\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (3.6.0)\n",
            "Collecting seqeval==0.0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/55/dd/3bf1c646c310daabae47fceb84ea9ab66df7f518a31a89955290d82b8100/seqeval-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (1.0.3)\n",
            "Collecting bert4keras==0.6.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/4c/0dfe26eeeb13b46dc1a74f57c7f9531cc0881c5a8efc9f31a058f0c70f61/bert4keras-0.6.5.tar.gz\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->kashgari==1.1.4) (1.12.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari==1.1.4) (2.2.5)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/0c/fede535ac576c03863c44bf2e0bf051fe21f5e10103631b6b6236ae446f3/keras-transformer-0.32.0.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari==1.1.4) (2019.12.20)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari==1.1.4) (1.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.1.0)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.12.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.18.1)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.15.31)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.8)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.7.2)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.0.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.15.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (4.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (46.0.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (3.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.16.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.4.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.51.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (3.10.0)\n",
            "Building wheels for collected packages: kashgari, keras-bert, keras-gpt-2, bert4keras, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for kashgari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kashgari: filename=kashgari-1.1.4-cp36-none-any.whl size=88070 sha256=68233fe8908843e7a4df61712a8434f9f78b84811fcfccb0ed016ac559ed7760\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fdcjdi2l/wheels/b0/42/7d/7e096aa7f533a5ccee6a31cd62bde62a6bde90116a864a9132\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=5c0b4e76c42eb6391dafeb31acc28ad6bed29775924a409e240b9d3aaab0800d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n",
            "  Building wheel for keras-gpt-2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-gpt-2: filename=keras_gpt_2-0.14.0-cp36-none-any.whl size=10525 sha256=e75bb69e45b2879c78854b073d5656211a1f960e09923edd7897c67eafdc19fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d8/06/ba8216a77a55b8ba4a5c3932c7df93e87eeaea83ced27822aa\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.6.5-cp36-none-any.whl size=34212 sha256=87f28133be014913264b9825a21117654cf7e3f88f4391c356b13f3347a6a82d\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/82/72/56a894ccb2337a25b679cbac552bf2f15f5e8e798a37313de6\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.32.0-cp36-none-any.whl size=13266 sha256=79ff70774a189051d93707d11aba23d615c7280c23354203a3b186975e46ba05\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f0/ce/82fa5d024d5ef8e263f26a50dcee23820efe245680ce9c922a\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=9dd6c35c09955ec13893a2e447d05fda9e2eaeea5eac34949d4ed8096b426ad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=499e8b0a6b3191a9240a5cb549a327f3fec57d48b0d79b771c37d310d3e285f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=867015eef2099e5c6eae0217a663353c50859a25f42b9ef95d237aacc49f0553\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=c56dfb3a7c57d620cfcce52782c99369985a754e220069a07e2d13015321bbb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=f5404048aa9db046511666854f5b12cf9762040af447eca4efa982e6f4a605a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=24da8a5285decdef9f7aae93037d141b1c25fad4c13eee5378c59d73905dd6d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built kashgari keras-bert keras-gpt-2 bert4keras keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, keras-gpt-2, seqeval, bert4keras, kashgari\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "Successfully installed bert4keras-0.6.5 kashgari-1.1.4 keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-gpt-2-0.14.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.32.0 numpy-1.16.4 seqeval-0.0.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt07RDElvhoU",
        "colab_type": "code",
        "outputId": "112ee1a2-d304-4875-8c87-e91fcb3ce593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/electra_tiny.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/electra_tiny.zip\n",
            "  inflating: bert_config_tiny.json   \n",
            "  inflating: __MACOSX/._bert_config_tiny.json  \n",
            "  inflating: model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.data-00000-of-00001  \n",
            "  inflating: model.ckpt-1000000.index  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.index  \n",
            "  inflating: model.ckpt-1000000.meta  \n",
            "  inflating: __MACOSX/._model.ckpt-1000000.meta  \n",
            "  inflating: vocab.txt               \n",
            "  inflating: __MACOSX/._vocab.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}